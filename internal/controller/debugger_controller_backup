package controller

import (
	"context"
	"fmt"
	"time"

	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"

	debuggerv1beta1 "test.local/hpp-pool-debug-operator/api/v1beta1"
)

// DebuggerReconciler reconciles a Debugger object
type DebuggerReconciler struct {
	client.Client
	Scheme *runtime.Scheme
}

// +kubebuilder:rbac:groups=debugger.test.local,resources=debuggers,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=debugger.test.local,resources=debuggers/status,verbs=get;update;patch
// +kubebuilder:rbac:groups=debugger.test.local,resources=debuggers/finalizers,verbs=update
// +kubebuilder:rbac:groups="",resources=pods,verbs=get;list;watch;create;delete

// Reconcile is part of the main kubernetes reconciliation loop which aims to
// move the current state of the cluster closer to the desired state.
func (r *DebuggerReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	log := log.FromContext(ctx)
	log.Info("Reconciling Debugger", "name", req.NamespacedName)

	// 1. Fetch the Debugger instance
	debugger := &debuggerv1beta1.Debugger{}
	if err := r.Get(ctx, req.NamespacedName, debugger); err != nil {
		if apierrors.IsNotFound(err) {
			// Request object not found, could have been deleted after reconcile request.
			log.Info("Debugger resource not found. Ignoring since object must be deleted")
			return ctrl.Result{}, nil
		}
		// Error reading the object - requeue the request.
		log.Error(err, "Failed to get Debugger")
		return ctrl.Result{}, err
	}

	// 2. List all Pods matching the selector in the specified namespace
	listOpts := []client.ListOption{
		client.InNamespace(debugger.Spec.Namespace),
		client.MatchingLabelsSelector{Selector: labels.Set(debugger.Spec.PodSelector.MatchLabels).AsSelector()},
	}

	targetPods := &corev1.PodList{}
	if err := r.List(ctx, targetPods, listOpts...); err != nil {
		log.Error(err, "Failed to list target Pods", "namespace", debugger.Spec.Namespace, "selector", debugger.Spec.PodSelector)
		return ctrl.Result{}, err
	}

	requeue := false
	for _, pod := range targetPods.Items {
		// 3. Check for CrashLoopBackOff status
		if isPodCrashing(&pod) {
			log.Info("Found crashing Pod", "name", pod.Name)
			
			// 4. Create or ensure the Debug Pod exists for this crashing Pod
			if err := r.ensureDebugPod(ctx, debugger, &pod); err != nil {
				log.Error(err, "Failed to ensure Debug Pod for", "targetPod", pod.Name)
				// Don't return error immediately, try to process other pods, but set requeue flag
				requeue = true 
			}
		}
	}

	if requeue {
		// If there was an error in ensuring a debug pod, retry soon.
		return ctrl.Result{RequeueAfter: 30 * time.Second}, nil
	}
    
	// Requeue to periodically check for new crashes
	return ctrl.Result{RequeueAfter: 60 * time.Second}, nil
}

// isPodCrashing checks if any container in the Pod is in CrashLoopBackOff
func isPodCrashing(pod *corev1.Pod) bool {
	if pod.Status.ContainerStatuses == nil {
		return false
	}
	for _, status := range pod.Status.ContainerStatuses {
		if status.State.Waiting != nil && status.State.Waiting.Reason == "CrashLoopBackOff" {
			return true
		}
	}
	return false
}

// ensureDebugPod checks if a debug pod exists for the targetPod and creates one if not.
func (r *DebuggerReconciler) ensureDebugPod(ctx context.Context, debugger *debuggerv1beta1.Debugger, targetPod *corev1.Pod) error {
	log := log.FromContext(ctx)
	debugPodName := fmt.Sprintf("%s-debug-%s", debugger.Name, targetPod.Name)
	debugPod := &corev1.Pod{}
	
	// Check if Debug Pod already exists
	err := r.Get(ctx, types.NamespacedName{Name: debugPodName, Namespace: debugger.Spec.Namespace}, debugPod)
	if err == nil {
		log.Info("Debug Pod already exists", "name", debugPodName)
		return nil // Already exists, nothing to do
	}
	if !apierrors.IsNotFound(err) {
		return err // Other error during Get
	}

	// Debug Pod does not exist, create it
	debugImage := debugger.Spec.DebugImage
	if debugImage == "" {
		// Fallback to a default image if not specified
		debugImage = "busybox"
	}
	
	// Define the Debug Pod object
	newDebugPod := &corev1.Pod{
		ObjectMeta: metav1.ObjectMeta{
			Name: debugPodName,
			Namespace: debugger.Spec.Namespace,
			Labels: map[string]string{
				"app.kubernetes.io/created-by": debugger.Name,
				"app.kubernetes.io/debugs": targetPod.Name,
			},
		},
		Spec: corev1.PodSpec{
			// Key: Run the debug pod on the same node as the crashing pod
			NodeName: targetPod.Spec.NodeName, 
			// Optional: Mount volumes from the target pod if needed for file access
			// Volumes: targetPod.Spec.Volumes, 
			Containers: []corev1.Container{
				{
					Name: "debugger-container",
					Image: debugImage,
					Command: debugger.Spec.Command, // The user-defined command
					// Optional: Mount paths (must match volumes)
					// VolumeMounts: targetPod.Spec.Containers[0].VolumeMounts, 
				},
			},
			RestartPolicy: corev1.RestartPolicyOnFailure, // Run once and exit
		},
	}

	// Set the Debugger instance as the owner and controller
	// This ensures the Debug Pod is deleted when the Debugger CR is deleted.
	if err := ctrl.SetControllerReference(debugger, newDebugPod, r.Scheme); err != nil {
		return err
	}

	log.Info("Creating Debug Pod", "name", debugPodName)
	if err = r.Create(ctx, newDebugPod); err != nil {
		return err
	}

	return nil
}

// SetupWithManager sets up the controller with the Manager.
func (r *DebuggerReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&debuggerv1beta1.Debugger{}).
		Owns(&corev1.Pod{}). // Watch Pods that are owned by the Debugger CR
		Complete(r)
}
